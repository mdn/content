---
title: Robots.txt
slug: Glossary/Robots.txt
tags:
  - Glossary
  - Infrastructure
---
Robots.txt is a file which is usually placed in the root of any website. It decides whether {{Glossary("crawler", "crawlers")}} are permitted or forbidden access to the web site.

For example, the site admin can forbid crawlers to visit a certain folder (and all the files therein contained) or to crawl a specific file, usually to prevent those files being indexed by other search engines.

## See also

- [Robots.txt](https://en.wikipedia.org/wiki/Robots.txt) on Wikipedia
- <https://developers.google.com/search/reference/robots_txt>
- Standard specification draft: [https://datatracker.ietf.org/doc/html/draft-rep-wg-topic](https://datatracker.ietf.org/doc/html/draft-rep-wg-topic-00)
- <https://www.robotstxt.org/>
